--- 
title: "TMA4315: Compulsory exercise 1 (title)" 
subtitle: "Group 0: Name1, Name2 (subtitle)" 
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output:
  bookdown::pdf_document2:
    keep_tex: yes
    toc: no
    number_sections: false
  html_document: default
  pdf_document: default
header-includes:
  - \usepackage{bm}
---

```{r setup, include = FALSE}
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, 
               warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

```{r}
rm(list=ls())

showsol <- FALSE

library(formatR)
library(knitr)
library(ggplot2)
library(tidyverse)
library(glue)      # fstring-like
library(doSNOW)    # Parallel computing
library(foreach)
```


# Part 1

**Bold**

_italic_

To get a pdf file, make comments of the lines with the "html_document" 
information, and make the lines with the "pdf_document" information regular, 
and vice versa.

## a)

The log-likelihood function for a binary regression model: 
$$\ell(\beta) = \sum_{i=1}^n y_i ln(\pi_i) + (1-y_i)ln(1-\pi_i)$$
How we arrived at this expression:
1. Start with the link function, aka logarithm of the odds, $ln(\frac{\pi}{1-\pi}) = \sum_{i=0}^n \beta_i x_{i}$, where $x_0=1$. This can be written as $\pi = \frac{1}{1+e^{-\sum_{i=0}^n \beta_i x_{i}}}$ 
2. Then we formulate the likelihood: $L(\beta) = \prod_{i=1}^n P(x_i | \beta) = \prod_{i=1}^n \left(\frac{1}{1 + \exp(-(\beta x_i))}\right)^{y_i} \left(1 - \frac{1}{1 + \exp(-(\beta x_i))}\right)^{1-y_i}$ (here $\beta$ and $x_i$ are vectors, and $y_i$ is the response-variable). 
3. We get the log-likelihood by taking the logarithm: $\ell(\beta) = \sum_{i=1}^n \left[ y_i \log\left(\frac{1}{1 + \exp(-(\beta x_i))}\right) + (1 - y_i) \log\left(1 - \frac{1}{1 + \exp(-(\beta x_i))}\right) \right]$
4. We put $\pi = \frac{1}{1 + \exp(-(\beta x_i))}$ and get $\ell(\beta) = \sum_{i=1}^n \left[ y_i \log\left(\pi_i\right) + (1 - y_i) \log\left(1 - \pi_i\right) \right]$. 

## b)

```{r}
# importing data
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
mount <- read.table(file = filepath, header = TRUE, col.names = c("height",
    "prominence", "fail", "success"))
```

```{r}
# fitting model
logreg.mod = glm(cbind(success, fail) ~ height + prominence, data = mount, family = "binomial")
summary(logreg.mod)
# perform likelihood ratio test
logreg.null_mod = glm(cbind(success, fail) ~ 1, data = mount, family = "binomial")
anova(logreg.null_mod, logreg.mod, "LRT")
# creating 95% confint for beta
cat(" \n Confidence interval: \n")
confint(logreg.mod)
cat(" \n exp(CI): \n")
exp(confint(logreg.mod))
```
* We can see from the estimates that the log(odds) decrease when height or prominence increase, meaning the chance of success decreases. 
* The p-values obtained by performed wald-test suggest that the covariates are very significant. We also performed LRT, where the decrease in residual deviance from 715.29 in Model 1 to 414.68 in Model 2 indicates that Model 2 fits the data significantly better.
* CI for height: (-0.0019157664 -1.359060e-03), which also clearly indicates a negative log-odds relation between height and success. 
* In we take $(exp(\beta_L), exp(\beta_H))$ we get the odds instead of the log-odds, which is given above under exp(CI). Here, since odds-confidence interval for height and prominence displays values <1, it indicates that for unit increase in height and prominence the odds for success diminishes. 

## c) 

```{r}
dev_res <- residuals(logreg.mod, type = "deviance")

# Create a data frame to use with ggplot2
plot_data <- data.frame(Height = logreg.mod$data$height, 
                        Prominence = logreg.mod$data$prominence, 
                        DevRes = dev_res)

# Plotting deviance residuals against height
ggplot(plot_data, aes(x = Height, y = DevRes)) +
  geom_point() +
  labs(x = "Height", y = "Deviance Residuals", 
       title = "Deviance Residuals vs Height")

# Plotting deviance residuals against prominence
ggplot(plot_data, aes(x = Prominence, y = DevRes)) +
  geom_point() +
  labs(x = "Prominence", y = "Deviance Residuals", 
       title = "Deviance Residuals vs Prominence")

```


# Part 2

```{r}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2023h/eliteserien2023.csv"
eliteserie <- read.csv(file = filepath)
eliteserie.played <- eliteserie %>% drop_na()
eliteserie.unplayed <- eliteserie[eliteserie %>% apply(1, function(row) any(is.na(row))), ]

NGames <- table(c(eliteserie.played$home[!is.na(eliteserie.played$yh)], eliteserie.played$away[!is.na(eliteserie.played$yh)]))
RangeofGames <- range(NGames)
```


##  a)
The null hypothesis is that the goals scored by a team is independent of wheather they are home or away, e.g. that the goals scored by the home- and away team are independently distributed. In this case we have two team categories which can score M goals, where $M = \max_{ij}{O_{ij}}$, $\mathbf{O} \in \mathbb{R}^{2\times M}$ is the matrix representing the contingency table. The matrix of expected frequencies, whose entries are  given by
\begin{equation*}
  E_{ij} = \frac{\sum_p{\mathbf{O}_{ip}} + \sum_k{\mathbf{O}_{kj}}}{\sum_{p, k}{\mathbf{O}_{pk}}}
\end{equation*}
can be written in matrix form as
\begin{equation}
  (\#eq:expfreq)
  \mathbf{E} = \frac{\mathbf{O}\bm{1}_M\bm{1}_2^T\mathbf{O}}{\bm{1}^T_2\mathbf{O}\bm{1}_M}
\end{equation}
where $\bm{1}_k \in \mathbb{R}^k$ is the k-dimensional vector of ones. Using the definition of $\mathbf{O}$ and Equation \@ref{eq:expfreq}, the $\chi^2$-test can be written
\begin{equation}
  \chi^2 = \sum_{i, j}{\frac{\mathbf{O}_{ij} - \mathbf{E}_{ij}}{\mathbf{E}_{ij}}}
\end{equation}
This value is distributed by $\chi^2_{M-1}$ under the null hypothesis.
```{r}
O <- rbind(table(eliteserie.played$yh), table(eliteserie.played$ya)) %>% as.matrix
O[2, (max(eliteserie.played$ya)+2):(max(eliteserie.played$yh)+1)]  <- 0

M <- max(eliteserie.played$yh) + 1
E <- c(O %*% rep(1, M)) %o% c(t(O) %*% rep(1, 2)) / c(rep(1, 2) %*% O %*% rep(1, M))
chi_sq <- sum((O - E)^2/E)
p.chi_sq <- pchisq(chi_sq, M-1, lower.tail=FALSE)
```
The p-value of the $\chi^2$ test is significant, and one can therefore conclude that the goals scored by the home- and away team are not independent. The assumption of independence is therefore not always reasonable.


# b)
```{r}
calculate_scores <- function(eliteserie) {
#   Given a dataframe from eliteserie, calculates the scores based on all matches
#   played so far.
#   
#   Args:
#     elitesere (DataFrame): Eliteserie dataframe with columns (home, away, yh, ya)
#   Returns:
#     (Float) Score
  df.scores <- inner_join(
    eliteserie %>% # Score from home- and away matches
      group_by(home) %>%
      summarize(score=3*sum(yh > ya) + 1*sum(yh == ya)),
    eliteserie %>%
      group_by(away) %>%
      summarize(score=3*sum(yh < ya) + 1*sum(yh == ya)),
    by=c("home" = "away")
  ) %>%
    mutate(score=score.x + score.y) %>%
    select(-c("score.x", "score.y")) %>%
    rename(team=home)
  
  return(df.scores)
}
```

```{r}
scores.played <- calculate_scores(eliteserie.played) %>%
  mutate(position=rank(-score, ties.method='min'))

scores.played %>% arrange(position)
```

TODO: Account for teams with equal score.

## c)
The first step is to transform the data into a suitable design matrix. In  this case, the score should be the response, the team should be a category, and whether it is away or home should be another category.

```{r}
eliteserie.played.transf <- eliteserie.played %>%
  gather(key='is_home', value='team', home, away) %>%
  gather(key='y_type', value='y', yh, ya) %>%
  filter((is_home == "home" & y_type == "yh") | (is_home == "away" & y_type == "ya")) %>%
  select(-c(y_type, X)) %>%
  mutate(is_home=ifelse(is_home == 'home', TRUE, FALSE))
```

Then we can implement the Fisher scoring algorithm. For the Poisson family (log-link), the socre and expected Fisher information is given  by
\begin{align}
  \bm{s}(\bm{\beta}) &= \sum_{i=1}^n{\left(y_i-\exp{(\bm{x}_i^T\beta)}\right)} \\
  \mathbf{F}(\bm{\beta}) &= \sum_{i=1}^n{\bm{x}_i\bm{x}_i^T \exp{(\bm{x}_i^T\beta)}}
\end{align}
and the Fisher-scoring algorithm is therefore
\begin{equation}
  \bm{\beta}^{(t+1)} = \bm{\beta}^{(t)} + \mathbf{F}(\bm{\beta}^{(t)})^{-1}\bm{s}(\bm{\beta}^{(t)})
\end{equation}
```{r}
fisher_score_iteration <- function(beta, X, y) {
  n <- ncol(X)
  s <- c(t(y - exp(X %*% beta)) %*% X)
  Fisher <- matrix(rep(0, n^2), nrow=n, ncol=n)
  for (i in 1:nrow(X)) {
    Fisher <- Fisher + c(exp(X[i, ] %*% beta)) * X[i, ] %o% X[i, ]
  }
  return(beta + solve(Fisher, s))
}
```

Reusing some of the code from `mylm`, we can easily obtain the design matrix $X$ and the response $y$, and run the Fisher scoring algorihtm. The algorithm converges very quickly, so 100 iterations should be more than safe. Alternatively, one can implement some stopping criterion based on the absolute difference in $\bm{\beta}^{(t)}$ or based on the likelihood function for $\bm{\beta}$.
```{r}
fit_coefficients <- function(formula, data, contrasts = NULL, ...) {
  # Extract model matrix & responses
  mf <- model.frame(formula = formula, data = data)
  X  <- model.matrix(attr(mf, "terms"), data = mf, contrasts.arg = contrasts)
  y  <- model.response(mf)
  terms <- attr(mf, "terms")

  beta <- rep(0, ncol(X))
  for(i in 1:100) {
    beta <- fisher_score_iteration(beta, X, y)
  }
  return(beta)
}
```

```{r}
my.coeffs <- fit_coefficients(y ~ team + is_home, data=eliteserie.played.transf)
my.coeffs
```


Asserting that they are indeed correct using the builtin glm function in R:
```{r}
ref.coeffs <- glm(y ~ team + is_home, data=eliteserie.played.transf, family='poisson')$coefficients
abs(mean(my.coeffs - ref.coeffs))
```

Looks good!

## d)
In order to simulate, we first need to add the coefficient of the reference level to `my.coeffs`.
```{r}
my.coeffs[['teamAalesund']] <- 0
```

Simulation can be done in the following steps:
1. Estimate $\hat{\lambda}$ for each team in each match using the model, i.e.
\begin{align*}
  \hat{\lambda_{i, H}} &= \exp{\left(\beta_0 + \beta_H + \beta_H - \beta_A \right)} \\
  \hat{\lambda_{i, A}} &= \exp{\left(\beta_0 + \beta_A - \beta_H \right)}
\end{align*}
2. For $m$ simulations, and for all $i$ matches, simulate the goals of each team assuming they follow a Poisson distribution with rate $\hat{\lambda}_{i, H}$ and $\hat{\lambda}_{i, A}$ for the home- and away teams  respectively.

We will first implement the function to estimate the rate for each match. Since I have created the model a bit differently, I will do this manually  by retrieving each respective coefficient.
```{r}
estimate_lambda <- function(match) {
  # Args:
  #   match (DataFrame row): Row of the eliteserie dataframe, ya and yh may be excluded
  # Returns:
  #   Same row but with lambda estimates.
  # Retrieving the coefficients
  coeff.home <- my.coeffs[[glue("team{match$home}")]]
  coeff.away <- my.coeffs[[glue("team{match$away}")]]
  
  # Calculating rates
  match$lambda.h <- exp(my.coeffs[["(Intercept)"]] + my.coeffs[["is_homeTRUE"]] + coeff.home - coeff.away)
  match$lambda.a <- exp(my.coeffs[["(Intercept)"]] + coeff.away - coeff.home)
  
  return(match)
}
```

```{r}
eliteserie.unplayed.est <- eliteserie.unplayed %>%
  rename(lambda.h=yh, lambda.a=ya) %>%
  {lapply(split(., seq_len(nrow(.))), estimate_lambda)} %>%
  {do.call(rbind, .)}
```

Now using the estimated rates to simulate 1000 matches.
```{r}
m <- 10000 # Number of  simulations

simulate_dataframe <- function() {
  eliteserie.unplayed.est %>%
    mutate(
      yh=rpois(n(), lambda.h),
      ya=rpois(n(), lambda.a)
    ) %>%
    select(-c(lambda.h, lambda.a))
}
```



Using the result to calculate the scoreboard for each simulation:
```{r}
# Simulating matches and calculating the scores for each dataframe using parallel computing:
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores, type="SOCK")
registerDoSNOW(cl)

scores.sim <- foreach(
  i=1:m,
  .packages='tidyverse'
) %dopar%
  {calculate_scores(rbind(eliteserie.played, simulate_dataframe()))} %>%
  bind_rows() %>%
  group_by(team) %>%
  summarize(scores=list(score), score.mean=mean(score), score.sd=sd(score))

stopCluster(cl)
```


Displaying the winner, on average
```{r}
scores.sim %>%
  mutate(position=rank(-score.mean, ties.method='min')) %>%
  arrange(position) %>%
  select(-scores)
```


Plotting score distributionhistograms for all teams:
```{r, fig.width=20, fig.height=12}
ggplot(scores.sim %>% unnest(cols=scores), aes(x=scores)) +
  geom_histogram(bins=50, fill="skyblue", color="black") +
  facet_wrap(~ team, scales="free_y") +
  labs(title="Score distribution for each team")
```

