---
title: "TMA4315: Compulsory exercise 1 (title)" 
subtitle: "Group 0: Name1, Name2 (subtitle)" 
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output:
  bookdown::pdf_document2:
    keep_tex: yes
    toc: no
    number_sections: false
header-includes:
  - \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r}
rm(list=ls())

showsol <- FALSE

library(formatR)
library(knitr)
library(ggplot2)
library(tidyverse)
library(glue)      # fstring-like
# library(doSNOW)    # Parallel computing
library(foreach)
```


# Part 1

## a)

The _log-likelihood function_ for a binary regression model: 
$$\ell(\beta) = \sum_{i=1}^n y_i ln(\pi_i) + (1-y_i)ln(1-\pi_i)$$
How we arrived at this expression:

**1.** Start with the link function, aka logarithm of the odds, $ln(\frac{\pi}{1-\pi}) = \sum_{i=0}^n \beta_i x_{i}$, where $x_0=1$. This can be written as $\pi = \frac{1}{1+e^{-\sum_{i=0}^n \beta_i x_{i}}}$ 

**2.** Then we formulate the likelihood: $L(\beta) = \prod_{i=1}^n P(x_i | \beta) = \prod_{i=1}^n \left(\frac{1}{1 + \exp(-(\beta x_i))}\right)^{y_i} \left(1 - \frac{1}{1 + \exp(-(\beta x_i))}\right)^{1-y_i}$ (here $\beta$ and $x_i$ are vectors, and $y_i$ is the response-variable). 

**3.** We get the log-likelihood by taking the logarithm: $\ell(\beta) = \sum_{i=1}^n \left[ y_i \log\left(\frac{1}{1 + \exp(-(\beta x_i))}\right) + (1 - y_i) \log\left(1 - \frac{1}{1 + \exp(-(\beta x_i))}\right) \right]$

**4.** We put $\pi = \frac{1}{1 + \exp(-(\beta x_i))}$ and get $\ell(\beta) = \sum_{i=1}^n \left[ y_i \log\left(\pi_i\right) + (1 - y_i) \log\left(1 - \pi_i\right) \right]$. 

**5.** To then get the Maximum likelihood estimate, we use an optimization method on the likelihood function to find the maximum and the likelihood function. To do this one often finds the zero of the derivative, which in our case is the score-function. 

## b)

```{r}
# importing data
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
mount <- read.table(file = filepath, header = TRUE, col.names = c("height",
    "prominence", "fail", "success"))
```

```{r}
# fitting model
logreg.mod = glm(cbind(success, fail) ~ height + prominence, data = mount, family = "binomial")
summary(logreg.mod)
# perform likelihood ratio test
logreg.null_mod = glm(cbind(success, fail) ~ 1, data = mount, family = "binomial")
anova(logreg.null_mod, logreg.mod, "LRT")
# creating 95% confint for beta
cat(" \n Confidence interval: \n")
confint(logreg.mod)
cat(" \n exp(CI): \n")
exp(confint(logreg.mod))
```

* We can see from the estimates that the log(odds) decrease when height or prominence increase, meaning the chance of success decreases. More generally, this means that when we multiply the coefficients with 1 unit, it signifies by how much the log(odds) are effected by the covariate that coefficient belongs to. 
* The p-values obtained by performed wald-test (which is how R calculates the p-values found in the summary, and is performed by using the formula $w = \left( C\hat{\beta} - d \right)^T \left[ C F^{-1}(\hat{\beta}) C^T \right]^{-1} \left( C\hat{\beta} - d \right)$, but when testing only one coefficient this becomes $\left( \hat{\beta}_k - \beta_k \right) \left[ a_{kk}(\hat{\beta}) \right]^{-1} \left( \hat{\beta}_k - \beta_k \right)$) suggest that the covariates are very significant. We also performed LRT, where the decrease in residual deviance from 715.29 in Model 1 to 414.68 in Model 2 indicates that Model 2 fits the data significantly better.
* CI for height: (-0.0019157664 -1.359060e-03), which also clearly indicates a negative log-odds relation between height and success.This CI are based on the likelihood function. They are more accurate than Wald intervals, especially for small sample sizes. The **confint()** function calculates these intervals by finding the values of the parameter for which the likelihood function takes a specific value. This method doesn’t rely on the normal approximation, and it’s often used when the sample size is small.
* If we take $(exp(\beta_L), exp(\beta_H))$ we get the odds instead of the log-odds, which is given above under exp(CI). Here, since odds-confidence interval for height and prominence displays values <1, it indicates that for unit increase in height and prominence the odds for success diminishes. 

## c) 

```{r}
dev_res <- residuals(logreg.mod, type = "deviance")

# Create a data frame to use with ggplot2
plot_data <- data.frame(Height = logreg.mod$data$height, 
                        Prominence = logreg.mod$data$prominence, 
                        DevRes = dev_res)

# Plotting deviance residuals against height
ggplot(plot_data, aes(x = Height, y = DevRes)) +
  geom_point() +
  labs(x = "Height", y = "Deviance Residuals", 
       title = "Deviance Residuals vs Height")

# Plotting deviance residuals against prominence
ggplot(plot_data, aes(x = Prominence, y = DevRes)) +
  geom_point() +
  labs(x = "Prominence", y = "Deviance Residuals", 
       title = "Deviance Residuals vs Prominence")

n = 100
# initializing height vector and prominence vector
min_height = min(mount$height)
max_heigth = max(mount$height)
min_prominence = min(mount$prominence)
max_prominence = max(mount$prominence)
seq_height = seq(from=min_height, to=max_heigth, length.out=n)
seq_prominence = seq(from=min_prominence, to=max_prominence, length.out=n)

# generating estimated probabilities matrix 
loglikmat <- matrix(NA, nrow = n, ncol = n)
loglikframe <- data.frame()
for (i in 1:length(seq_height)){
  for (j in 1:length(seq_prominence)){
    loglikmat[i,j] <- predict(logreg.mod, newdata = data.frame(height = c(seq_height[i]), prominence = c(seq_prominence[j])), type = "response")
    loglikframe <- rbind(loglikframe, c(seq_height[i], seq_prominence[j], loglikmat[i,j]))
  }
}
names(loglikframe) <- c("Height", "Prominence", "Prob")
head(loglikframe)

# plotting 
ggplot(data = loglikframe, aes(x = Height, y = Prominence, z = Prob)) + geom_raster(aes(fill = Prob)) +
  geom_point(data = loglikframe[which.max(loglikframe$prob),], mapping = aes(x = Height, y = Prominence), 
             size = 5, col = "red", shape = 21, stroke = 2) + scale_shape(solid = FALSE) + geom_contour(col = "black") 

```

There does not seem to be any significant pattern when plotting the deviance residuals against the covariates. They seem to be randomly distributed around 0. There is a higher concentration of residuals in the lower heights, but that is only because there a less samples of people climbing the highest mountains. 

From the plot of the probabilities as a function of height and prominence we see what we expected, namely that the higher the Height and prominence of the mountain, the smaller the chance of success. 


## d) 
We have that as the number of samples $n$ where $n \rightarrow \infty$, and then $ln(\frac{p}{1-p}) = y = \sum^q_{i=0} x_i\beta_i \sim N(x^T\beta, F^{-1}(\beta))$. We construct a confidence interval for $y$, and then transform the interval. We have: 
$$ 
N(0,1) \sim \frac{y - x^T\beta}{\sqrt{a_{kk}}}
$$
where $a_{kk} = (F^{-1}(\beta))_{kk}$.

Which gives us the confidence interval 
$$
\left( x^T\beta - z_{\alpha/2} \sqrt{a_{kk}} \; , \; x^T\beta + z_{\alpha/2} \sqrt{a_{kk}}  \right)
$$
Transforming this interval so we get the probability instead of log-odds we get the interval: 
$$
\left( \frac{e^{x^T\beta - z_{\alpha/2} \sqrt{a_{kk}}}}{1 + e^{x^T\beta - z_{\alpha/2} \sqrt{a_{kk}}}} \; , \; \frac{e^{x^T\beta + z_{\alpha/2} \sqrt{a_{kk}}}}{1 + e^{x^T\beta + z_{\alpha/2} \sqrt{a_{kk}}}} \right)
$$
We do the calculations for the CI in R:
```{r}
std_errors <- c(1.064, 0.0001420, 0.00004554)

# Z-score for 95% confidence interval
z_score <- 1.96

# Mountain data
mountains <- list(
  Everest = c(1, 8848, 8848),
  Chogolisa = c(1, 7665, 1624)
)

# Function to calculate results
calculate_results <- function(features, coefficients, std_errors, z_score) {
  # Calculate linear predictor
  eta <- sum(features * coefficients)
  
  # Transform to probability
  p_hat <- exp(eta) / (1 + exp(eta))
  
  # Calculate standard error of the linear predictor
  se_eta <- sqrt(sum((features * std_errors)^2))
  
  # Confidence interval for linear predictor
  ci_eta <- c(eta - z_score * se_eta, eta + z_score * se_eta)
  
  # Transform confidence interval to probability scale
  ci_p <- exp(ci_eta) / (1 + exp(ci_eta))
  
  return(list(eta = eta, p_hat = p_hat, se_eta = se_eta, ci_eta = ci_eta, ci_p = ci_p))
}

# Calculate and print results for both mountains
results <- lapply(mountains, calculate_results, logreg.mod$coefficients, std_errors, z_score)
print(results)
```

The model estimates a 9% success rate for ascending Mount Everest, with a 95% confidence interval ranging from 0.4% to 73.2%. This wide confidence interval reflects a high level of uncertainty, likely due to the extreme values of height and prominence for Everest as compared to the mountains in the training data.

For Chogolisa, the estimated success rate is 71%, with a 95% confidence interval from 10.8% to 97.9%. Though the confidence interval is also wide, the point estimate is closer to the empirical success rate of 91%. 


# Part 2
```{r}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2023h/eliteserien2023.csv"
eliteserie <- read.csv(file = filepath)
eliteserie.played <- eliteserie %>% drop_na()
eliteserie.unplayed <- eliteserie[eliteserie %>% apply(1, function(row) any(is.na(row))), ]

NGames <- table(c(eliteserie.played$home[!is.na(eliteserie.played$yh)], eliteserie.played$away[!is.na(eliteserie.played$yh)]))
RangeofGames <- range(NGames)
```


##  a)
The null hypothesis is that the goals scored by a team is independent of whether they are home or away, i.e. that the goals scored by the home- and away team are independently distributed. In this case we have two team categories which can score M goals, where $M = \max_{ij}{O_{ij}}$, $\mathbf{O} \in \mathbb{R}^{2\times M}$ is the matrix representing the contingency table. The matrix of expected frequencies, whose entries are  given by
\begin{equation*}
  E_{ij} = \frac{\sum_p{\mathbf{O}_{ip}} + \sum_k{\mathbf{O}_{kj}}}{\sum_{p, k}{\mathbf{O}_{pk}}}
\end{equation*}
can be written in matrix form as
\begin{equation}
  (\#eq:expfreq)
  \mathbf{E} = \frac{\mathbf{O}\bm{1}_M\bm{1}_2^T\mathbf{O}}{\bm{1}^T_2\mathbf{O}\bm{1}_M}
\end{equation}
where $\bm{1}_k \in \mathbb{R}^k$ is the k-dimensional vector of ones. Using the definition of $\mathbf{O}$ and Equation \@ref{eq:expfreq}, the $\chi^2$-test can be written
\begin{equation}
  \chi^2 = \sum_{i, j}{\frac{\mathbf{O}_{ij} - \mathbf{E}_{ij}}{\mathbf{E}_{ij}}}
\end{equation}
This value is distributed by $\chi^2_{M-1}$ under the null hypothesis.
```{r}
O <- rbind(table(eliteserie.played$yh), table(eliteserie.played$ya)) %>% as.matrix
O[2, (max(eliteserie.played$ya)+2):(max(eliteserie.played$yh)+1)]  <- 0

M <- max(eliteserie.played$yh) + 1
E <- c(O %*% rep(1, M)) %o% c(t(O) %*% rep(1, 2)) / c(rep(1, 2) %*% O %*% rep(1, M))
chi_sq <- sum((O - E)^2/E)
p.chi_sq <- pchisq(chi_sq, M-1, lower.tail=FALSE)
p.chi_sq
```
The p-value of the $\chi^2$ test is significant at $~0.017$, and one can therefore conclude that the goals scored by the home- and away team are not independent. The assumption of independence is therefore not always reasonable.


# b)
```{r}
calculate_scores <- function(eliteserie) {
#   Given a dataframe from eliteserie, calculates the scores based on all matches
#   played so far.
#   
#   Args:
#     elitesere (DataFrame): Eliteserie dataframe with columns (home, away, yh, ya)
#   Returns:
#     (Float) Score
  df.scores <- inner_join(
    eliteserie %>% # Score from home- and away matches
      group_by(home) %>%
      summarize(score=3*sum(yh > ya) + 1*sum(yh == ya)),
    eliteserie %>%
      group_by(away) %>%
      summarize(score=3*sum(yh < ya) + 1*sum(yh == ya)),
    by=c("home" = "away")
  ) %>%
    mutate(score=score.x + score.y) %>%
    select(-c("score.x", "score.y")) %>%
    rename(team=home)
  
  return(df.scores)
}
```

```{r}
scores.played <- calculate_scores(eliteserie.played) %>%
  mutate(position=rank(-score, ties.method='min'))

scores.played %>% arrange(position)
```

The only teams sharing scores are Haugesund, Sandefjord Fotball, and Vålerenga, each with 21 points.
```{r}
num_goals <- {inner_join(
  eliteserie.played %>% # Score from home- and away matches
    group_by(home) %>%
    summarize(goals=sum(yh)),
  eliteserie.played %>%
    group_by(away) %>%
    summarize(goals=sum(ya)),
  by=c("home" = "away")
)} %>%
  mutate(goals=goals.x + goals.y) %>%
  select(-c(goals.x, goals.y))

num_goals[c(5, 10, 16), ]
```
Thus, 12th place goes to Sandefjord Fotball, 13th place goes to Vålerenga, and 14th place goes to Haugesund.

## c)
We want a model on the form

\begin{equation*}
  \ln{\mathbb{E}(Y)} = \beta_0 + \beta_{\mathrm{home}}x_{\mathrm{home}} + \mathbf{X}_{\mathrm{teams}}\bm{\beta}_{\mathrm{teams}}
\end{equation*}
where $\mathbf{X}_{\mathrm{teams}}$ is the sub matrix of the design matrix including the one-hot encoding for all teams, and $\bm{\beta}_{\mathrm{teams}}$ are the strength coefficients for each team.

The first step to fit the model is to transform the data into a suitable design matrix. In  this case, the score should be the response, the team should be a category, and whether it is away or home should be another category. The first step to fit the model is to transform the data into a suitable design matrix. R will naturally handle the issue of linear dependence in the design matrix by making Aalesund a reference level.

```{r}
eliteserie.played.transf <- eliteserie.played %>%
  gather(key='is_home', value='team', home, away) %>%
  gather(key='y_type', value='y', yh, ya) %>%
  filter((is_home == "home" & y_type == "yh") | (is_home == "away" & y_type == "ya")) %>%
  select(-c(y_type, X)) %>%
  mutate(is_home=ifelse(is_home == 'home', TRUE, FALSE))
```

Then we can implement the Fisher scoring algorithm. For the Poisson family (log-link), the socre and expected Fisher information is given  by
\begin{align}
  \bm{s}(\bm{\beta}) &= \sum_{i=1}^n{\left(y_i-\exp{(\bm{x}_i^T\beta)}\right)} \\
  \mathbf{F}(\bm{\beta}) &= \sum_{i=1}^n{\bm{x}_i\bm{x}_i^T \exp{(\bm{x}_i^T\beta)}}
\end{align}
and the Fisher-scoring algorithm is therefore
\begin{equation}
  \bm{\beta}^{(t+1)} = \bm{\beta}^{(t)} + \mathbf{F}(\bm{\beta}^{(t)})^{-1}\bm{s}(\bm{\beta}^{(t)})
\end{equation}
```{r}
fisher_score_iteration <- function(beta, X, y) {
  n <- ncol(X)
  s <- c(t(y - exp(X %*% beta)) %*% X)
  Fisher <- matrix(rep(0, n^2), nrow=n, ncol=n)
  for (i in 1:nrow(X)) { # This is not ideal, but multidimensional arrays are difficult to manage in R
    Fisher <- Fisher + c(exp(X[i, ] %*% beta)) * X[i, ] %o% X[i, ]
  }
  return(beta + solve(Fisher, s))
}
```

Reusing some of the code from `mylm`, we can easily obtain the design matrix $X$ and the response $y$, and run the Fisher scoring algorithm. The algorithm converges very quickly, so 100 iterations should be more than safe. Alternatively, one can implement some stopping criterion based on the absolute difference in $\bm{\beta}^{(t)}$, or based on the likelihood function for $\bm{\beta}$.
```{r}
fit_coefficients <- function(formula, data, contrasts = NULL, ...) {
  # Extract model matrix & responses
  mf <- model.frame(formula = formula, data = data)
  X  <- model.matrix(attr(mf, "terms"), data = mf, contrasts.arg = contrasts)
  y  <- model.response(mf)
  terms <- attr(mf, "terms")

  beta <- rep(0, ncol(X))
  for(i in 1:100) {
    beta <- fisher_score_iteration(beta, X, y)
  }
  return(beta)
}
```

This leads to the following coefficient estimates:
```{r}
my.coeffs <- fit_coefficients(y ~ team + is_home, data=eliteserie.played.transf)
my.coeffs
```


Asserting that they are indeed correct using the built in glm function in R:
```{r}
ref.coeffs <- glm(y ~ team + is_home, data=eliteserie.played.transf, family='poisson')$coefficients
abs(mean(my.coeffs - ref.coeffs))
```
Looks good!

## d)
In order to simulate 1000 matches, we first need to add the coefficient of the reference level to `my.coeffs`.
```{r}
ref_coeff <- eliteserie$home %>% 
  unique() %>% 
  lapply(function(str) glue("team{str}")) %>% 
  unlist() %>% 
  setdiff(names(my.coeffs))

my.coeffs[[ref_coeff]] <- 0
```

Simulation can be done in the following steps:
1. Estimate $\hat{\lambda}$ for each team in each match using the model, i.e.
\begin{align*}
  \hat{\lambda_{i, H}} &= \exp{\left(\beta_0 + \beta_H + \beta_H - \beta_A \right)} \\
  \hat{\lambda_{i, A}} &= \exp{\left(\beta_0 + \beta_A - \beta_H \right)}
\end{align*}
2. For $m$ simulations, and for all $i$ matches, simulate the goals of each team assuming they follow a Poisson distribution with rate $\hat{\lambda}_{i, H}$ and $\hat{\lambda}_{i, A}$ for the home- and away teams  respectively.

We will first implement the function to estimate the rate for each match. Since I have created the model a bit differently, I will do this manually  by retrieving each respective coefficient.
```{r}
estimate_lambda <- function(match) {
  # Args:
  #   match (DataFrame row): Row of the eliteserie dataframe, ya and yh may be excluded
  # Returns:
  #   Same row but with lambda estimates.
  # Retrieving the coefficients
  coeff.home <- my.coeffs[[glue("team{match$home}")]]
  coeff.away <- my.coeffs[[glue("team{match$away}")]]
  
  # Calculating rates
  match$lambda.h <- exp(my.coeffs[["(Intercept)"]] + my.coeffs[["is_homeTRUE"]] + coeff.home - coeff.away)
  match$lambda.a <- exp(my.coeffs[["(Intercept)"]] + coeff.away - coeff.home)
  
  return(match)
}
```

Now applying the above function to each row, giving us coefficient estimates of $\lambda$ for each team in every match.
```{r}
eliteserie.unplayed.est <- eliteserie.unplayed %>%
  rename(lambda.h=yh, lambda.a=ya) %>%
  {lapply(split(., seq_len(nrow(.))), estimate_lambda)} %>%
  {do.call(rbind, .)}
```

Finally, using the estimated rates in `eliteserie.unplayed.est` to simulate 1 000 matches, and then calculate the scroeboard for all simulations.
```{r}
m <- 1000 # Number of  simulations

simulate_dataframe <- function() {
  eliteserie.unplayed.est %>%
    mutate(
      yh=rpois(n(), lambda.h),
      ya=rpois(n(), lambda.a)
    ) %>%
    select(-c(lambda.h, lambda.a))
}
```

```{r}
# num_cores <- parallel::detectCores() - 1
# cl <- makeCluster(num_cores, type="SOCK")
# registerDoSNOW(cl)

scores.sim <- foreach(
  i=1:m,
  .packages='tidyverse'
) %do%
  {calculate_scores(rbind(eliteserie.played, simulate_dataframe()))} %>%
  bind_rows() %>%
  group_by(team) %>%
  summarize(scores=list(score), score.mean=mean(score), score.sd=sd(score))

# stopCluster(cl)
```


With all realizations being saved in the dataframe `scores.sim`, we can easily summarize the result.
```{r}
scores.sim %>%
  mutate(position=rank(-score.mean, ties.method='min')) %>%
  select(-scores) %>%
  merge(scores.played[c('team', 'position')]  %>% rename(position.played=position), by='team') %>%
  mutate(pos_change=position.played - position) %>%
  select(-position.played) %>%
  arrange(position)
```
It is clear that Viking and Bodø/Glimt are leading by a substantial margin. Furthermore, most teams kept their positions as of October 1st, except for Molde and Brann, who switched. The models ability to estimate change in leaderboard based on the teams playing in the future is probably its best strength. The new leaderboard also appropriately positioned Haugesund, Vålerenga, and Sandefjord.

One can also observe that the estimated standard deviation starts out quite small for the worst teams, and steadily increases with increasing score. This is due to the fact that the last 80 matches (33% of the championship) are simulated using a Poisson distribution, and the rest of the data for calculating the score is identical for all simulations. Let $S_i$ be the score for team $i$. The total variance for team $i$ is given by

\begin{equation}
  (\#eq:var)
  \mathrm{Var}(S_i) = \mathrm{Var}(\mathrm{Played}_i) + \mathrm{Var}(\mathrm{Simulated}_i) = 0 + \hat{\lambda}_i = \hat{\lambda}_i
\end{equation}

Where we in \@ref(eq:var) used the fact that the variance of observed variables is obviously 0. And thus, the standard deviation for each team is actually the square root of their expected score increase during the last eighty matches (according to the simulation).

Plotting score distribution histograms for all teams:
```{r, fig.width=20, fig.height=12}
ggplot(scores.sim %>% unnest(cols=scores), aes(x=scores)) +
  geom_histogram(bins=60, fill="skyblue", color=NA) +
  facet_wrap(~ team, scales="free_y") +
  labs(title="Score distribution for each team")
```
The plots above show that the simulated score for each team is approximately normal.
