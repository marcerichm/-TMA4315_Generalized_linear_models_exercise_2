--- 
title: "TMA4315: Compulsory exercise 1 (title)" 
subtitle: "Group 0: Name1, Name2 (subtitle)" 
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output:
  bookdown::pdf_document2:
    keep_tex: yes
    toc: no
    number_sections: false
  html_document: default
  pdf_document: default
header-includes:
  - \usepackage{bm}
---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
library(ggplot2)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, 
               warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

```{r}
library(tidyverse)
```


# Part 1

**Bold**

_italic_

To get a pdf file, make comments of the lines with the "html_document" 
information, and make the lines with the "pdf_document" information regular, 
and vice versa.

## a)

The log-likelihood function for a binary regression model: 
$$\ell(\beta) = \sum_{i=1}^n y_i ln(\pi_i) + (1-y_i)ln(1-\pi_i)$$
How we arrived at this expression:
1. Start with the link function, aka logarithm of the odds, $ln(\frac{\pi}{1-\pi}) = \sum_{i=0}^n \beta_i x_{i}$, where $x_0=1$. This can be written as $\pi = \frac{1}{1+e^{-\sum_{i=0}^n \beta_i x_{i}}}$ 
2. Then we formulate the likelihood: $L(\beta) = \prod_{i=1}^n P(x_i | \beta) = \prod_{i=1}^n \left(\frac{1}{1 + \exp(-(\beta x_i))}\right)^{y_i} \left(1 - \frac{1}{1 + \exp(-(\beta x_i))}\right)^{1-y_i}$ (here $\beta$ and $x_i$ are vectors, and $y_i$ is the response-variable). 
3. We get the log-likelihood by taking the logarithm: $\ell(\beta) = \sum_{i=1}^n \left[ y_i \log\left(\frac{1}{1 + \exp(-(\beta x_i))}\right) + (1 - y_i) \log\left(1 - \frac{1}{1 + \exp(-(\beta x_i))}\right) \right]$
4. We put $\pi = \frac{1}{1 + \exp(-(\beta x_i))}$ and get $\ell(\beta) = \sum_{i=1}^n \left[ y_i \log\left(\pi_i\right) + (1 - y_i) \log\left(1 - \pi_i\right) \right]$. 

## b)

```{r}
# importing data
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
mount <- read.table(file = filepath, header = TRUE, col.names = c("height",
    "prominence", "fail", "success"))
```

```{r}
# fitting model
logreg.mod = glm(cbind(success, fail) ~ height + prominence, data = mount, family = "binomial")
summary(logreg.mod)
# perform likelihood ratio test
logreg.null_mod = glm(cbind(success, fail) ~ 1, data = mount, family = "binomial")
anova(logreg.null_mod, logreg.mod, "LRT")
# creating 95% confint for beta
cat(" \n Confidence interval: \n")
confint(logreg.mod)
cat(" \n exp(CI): \n")
exp(confint(logreg.mod))
```
* We can see from the estimates that the log(odds) decrease when height or prominence increase, meaning the chance of success decreases. 
* The p-values obtained by performed wald-test suggest that the covariates are very significant. We also performed LRT, where the decrease in residual deviance from 715.29 in Model 1 to 414.68 in Model 2 indicates that Model 2 fits the data significantly better.
* CI for height: (-0.0019157664 -1.359060e-03), which also clearly indicates a negative log-odds relation between height and success. 
* In we take $(exp(\beta_L), exp(\beta_H))$ we get the odds instead of the log-odds, which is given above under exp(CI). Here, since odds-confidence interval for height and prominence displays values <1, it indicates that for unit increase in height and prominence the odds for success diminishes. 

## c) 

```{r}
dev_res <- residuals(logreg.mod, type = "deviance")

# Create a data frame to use with ggplot2
plot_data <- data.frame(Height = logreg.mod$data$height, 
                        Prominence = logreg.mod$data$prominence, 
                        DevRes = dev_res)

# Plotting deviance residuals against height
ggplot(plot_data, aes(x = Height, y = DevRes)) +
  geom_point() +
  labs(x = "Height", y = "Deviance Residuals", 
       title = "Deviance Residuals vs Height")

# Plotting deviance residuals against prominence
ggplot(plot_data, aes(x = Prominence, y = DevRes)) +
  geom_point() +
  labs(x = "Prominence", y = "Deviance Residuals", 
       title = "Deviance Residuals vs Prominence")

```


# Part 2

```{r}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2023h/eliteserien2023.csv"
eliteserie <- read.csv(file = filepath) %>% drop_na()

NGames <- table(c(eliteserie$home[!is.na(eliteserie$yh)], eliteserie$away[!is.na(eliteserie$yh)]))
RangeofGames <- range(NGames)
```


##  a)
The null hypothesis is that the goals scored by a team is independent of wheather they are home or away, e.g. that the goals scored by the home- and away team are independently distributed. In this case we have two team categories which can score M goals, where $M = \max_{ij}{O_{ij}}$, $\mathbf{O} \in \mathbb{R}^{2\times M}$ is the matrix representing the contingency table. The matrix of expected frequencies, whose entries are  given by
\begin{equation*}
  E_{ij} = \frac{\sum_p{\mathbf{O}_{ip}} + \sum_k{\mathbf{O}_{kj}}}{\sum_{p, k}{\mathbf{O}_{pk}}}
\end{equation*}
can be written in matrix form as
\begin{equation}
  (\#eq:expfreq)
  \mathbf{E} = \frac{\mathbf{O}\bm{1}_M\bm{1}_2^T\mathbf{O}}{\bm{1}^T_2\mathbf{O}\bm{1}_M}
\end{equation}
where $\bm{1}_k \in \mathbb{R}^k$ is the k-dimensional vector of ones. Using the definition of $\mathbf{O}$ and Equation \@ref{eq:expfreq}, the $\chi^2$-test can be written
\begin{equation}
  \chi^2 = \sum_{i, j}{\frac{\mathbf{O}_{ij} - \mathbf{E}_{ij}}{\mathbf{E}_{ij}}}
\end{equation}
This value is distributed by $\chi^2_{M-1}$ under the null hypothesis.
```{r}
O <- rbind(table(eliteserie$yh), table(eliteserie$ya)) %>% as.matrix
O[2, (max(eliteserie$ya)+2):(max(eliteserie$yh)+1)]  <- 0

M <- max(eliteserie$yh) + 1
E <- c(O %*% rep(1, M)) %o% c(t(O) %*% rep(1, 2)) / c(rep(1, 2) %*% O %*% rep(1, M))
chi_sq <- sum((O - E)^2/E)
p.chi_sq <- pchisq(chi_sq, M-1, lower.tail=FALSE)
```
The p-value of the $\chi^2$ test is significant, and one can therefore conclude that the goals scored by the home- and away team are not independent. The assumption of independence is therefore not always reasonable.


# b)
```{r}
df.scores <- inner_join(
  eliteserie %>% # Score from home- and away matches
    group_by(home) %>%
    summarize(score=3*sum(yh > ya) + 1*sum(yh == ya)),
  eliteserie %>%
    group_by(away) %>%
    summarize(score=3*sum(yh < ya) + 1*sum(yh == ya)),
  by=c("home" = "away")
) %>%
  mutate(score=score.x + score.y) %>%
  select(-c("score.x", "score.y"))
```

```{r}
df.scores
```

TODO: Account for teams with equal score.

## c)
The first step is to transform the data into a suitable design matrix. In  this case, the score should be the response, the team should be a category, and wheather it is away or home should be another category.

```{r}
glm.data <- eliteserie %>%
  gather(key='is_home', value='team', home, away) %>%
  gather(key='y_type', value='y', yh, ya) %>%
  filter((is_home == "home" & y_type == "yh") | (is_home == "away" & y_type == "ya")) %>%
  select(-c(y_type, X)) %>%
  mutate(is_home=ifelse(is_home == 'home', TRUE, FALSE))
```
```{r}
fisher_score_iteration <- function(beta, X, y) {
  s <- t(y - exp(X %*% beta)) %*% X
  F <- matrix(nrow=ncol(X), ncol=ncol(X))
  for (i in 1:nrow(X)) {
    F <- F + c(exp(X[i, ] %*% beta)) * X[i, ] %o% X[i, ]
  }
  return(beta + solve(F, s))
}
```


```{r}
fit_coefficients <- function(formula, data, contrasts = NULL, ...) {
  # Extract model matrix & responses
  mf <- model.frame(formula = formula, data = data)
  X  <- model.matrix(attr(mf, "terms"), data = mf, contrasts.arg = contrasts)
  y  <- model.response(mf)
  terms <- attr(mf, "terms")

  return(fisher_score_iteration(rep(0.5, ncol(X)), X, y))
}
```

```{r}
fit_coefficients(y ~ team + is_home, data=glm.data)
```

