---
title: "TMA4315: Compulsory exercise 1 (title)" 
subtitle: "Group 0: Name1, Name2 (subtitle)" 
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output:
  bookdown::pdf_document2:
    keep_tex: yes
    toc: no
    number_sections: false
header-includes:
  - \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r}
rm(list=ls())

showsol <- FALSE

library(formatR)
library(knitr)
library(ggplot2)
library(tidyverse)
library(glue)      # fstring-like
library(doSNOW)    # Parallel computing
library(foreach)
```


# Part 1



# Part 2
```{r}
filepath <- "https://www.math.ntnu.no/emner/TMA4315/2023h/eliteserien2023.csv"
eliteserie <- read.csv(file = filepath)
eliteserie.played <- eliteserie %>% drop_na()
eliteserie.unplayed <- eliteserie[eliteserie %>% apply(1, function(row) any(is.na(row))), ]

NGames <- table(c(eliteserie.played$home[!is.na(eliteserie.played$yh)], eliteserie.played$away[!is.na(eliteserie.played$yh)]))
RangeofGames <- range(NGames)
```


##  a)
The null hypothesis is that the goals scored by a team is independent of whether they are home or away, i.e. that the goals scored by the home- and away team are independently distributed. In this case we have two team categories which can score M goals, where $M = \max_{ij}{O_{ij}}$, $\mathbf{O} \in \mathbb{R}^{2\times M}$ is the matrix representing the contingency table. The matrix of expected frequencies, whose entries are  given by
\begin{equation*}
  E_{ij} = \frac{\sum_p{\mathbf{O}_{ip}} + \sum_k{\mathbf{O}_{kj}}}{\sum_{p, k}{\mathbf{O}_{pk}}}
\end{equation*}
can be written in matrix form as
\begin{equation}
  (\#eq:expfreq)
  \mathbf{E} = \frac{\mathbf{O}\bm{1}_M\bm{1}_2^T\mathbf{O}}{\bm{1}^T_2\mathbf{O}\bm{1}_M}
\end{equation}
where $\bm{1}_k \in \mathbb{R}^k$ is the k-dimensional vector of ones. Using the definition of $\mathbf{O}$ and Equation \@ref{eq:expfreq}, the $\chi^2$-test can be written
\begin{equation}
  \chi^2 = \sum_{i, j}{\frac{\mathbf{O}_{ij} - \mathbf{E}_{ij}}{\mathbf{E}_{ij}}}
\end{equation}
This value is distributed by $\chi^2_{M-1}$ under the null hypothesis.
```{r}
O <- rbind(table(eliteserie.played$yh), table(eliteserie.played$ya)) %>% as.matrix
O[2, (max(eliteserie.played$ya)+2):(max(eliteserie.played$yh)+1)]  <- 0

M <- max(eliteserie.played$yh) + 1
E <- c(O %*% rep(1, M)) %o% c(t(O) %*% rep(1, 2)) / c(rep(1, 2) %*% O %*% rep(1, M))
chi_sq <- sum((O - E)^2/E)
p.chi_sq <- pchisq(chi_sq, M-1, lower.tail=FALSE)
p.chi_sq
```
The p-value of the $\chi^2$ test is significant at $~0.017$, and one can therefore conclude that the goals scored by the home- and away team are not independent. The assumption of independence is therefore not always reasonable.


# b)
```{r}
calculate_scores <- function(eliteserie) {
#   Given a dataframe from eliteserie, calculates the scores based on all matches
#   played so far.
#   
#   Args:
#     elitesere (DataFrame): Eliteserie dataframe with columns (home, away, yh, ya)
#   Returns:
#     (Float) Score
  df.scores <- inner_join(
    eliteserie %>% # Score from home- and away matches
      group_by(home) %>%
      summarize(score=3*sum(yh > ya) + 1*sum(yh == ya)),
    eliteserie %>%
      group_by(away) %>%
      summarize(score=3*sum(yh < ya) + 1*sum(yh == ya)),
    by=c("home" = "away")
  ) %>%
    mutate(score=score.x + score.y) %>%
    select(-c("score.x", "score.y")) %>%
    rename(team=home)
  
  return(df.scores)
}
```

```{r}
scores.played <- calculate_scores(eliteserie.played) %>%
  mutate(position=rank(-score, ties.method='min'))

scores.played %>% arrange(position)
```

The only teams sharing scores are Haugesund, Sandefjord Fotball, and Vålerenga, each with 21 points.
```{r}
num_goals <- {inner_join(
  eliteserie.played %>% # Score from home- and away matches
    group_by(home) %>%
    summarize(goals=sum(yh)),
  eliteserie.played %>%
    group_by(away) %>%
    summarize(goals=sum(ya)),
  by=c("home" = "away")
)} %>%
  mutate(goals=goals.x + goals.y) %>%
  select(-c(goals.x, goals.y))

num_goals[c(5, 10, 16), ]
```
Thus, 12th place goes to Sandefjord Fotball, 13th place goes to Vålerenga, and 14th place goes to Haugesund.

## c)
We want a model on the form

\begin{equation*}
  \ln{\mathbb{E}(Y)} = \beta_0 + \beta_{\mathrm{home}}x_{\mathrm{home}} + \mathbf{X}_{\mathrm{teams}}\bm{\beta}_{\mathrm{teams}}
\end{equation*}
where $\mathbf{X}_{\mathrm{teams}}$ is the sub matrix of the design matrix including the one-hot encoding for all teams, and $\bm{\beta}_{\mathrm{teams}}$ are the strength coefficients for each team.

The first step to fit the model is to transform the data into a suitable design matrix. In  this case, the score should be the response, the team should be a category, and whether it is away or home should be another category. The first step to fit the model is to transform the data into a suitable design matrix. R will naturally handle the issue of linear dependence in the design matrix by making Aalesund a reference level.

```{r}
eliteserie.played.transf <- eliteserie.played %>%
  gather(key='is_home', value='team', home, away) %>%
  gather(key='y_type', value='y', yh, ya) %>%
  filter((is_home == "home" & y_type == "yh") | (is_home == "away" & y_type == "ya")) %>%
  select(-c(y_type, X)) %>%
  mutate(is_home=ifelse(is_home == 'home', TRUE, FALSE))
```

Then we can implement the Fisher scoring algorithm. For the Poisson family (log-link), the socre and expected Fisher information is given  by
\begin{align}
  \bm{s}(\bm{\beta}) &= \sum_{i=1}^n{\left(y_i-\exp{(\bm{x}_i^T\beta)}\right)} \\
  \mathbf{F}(\bm{\beta}) &= \sum_{i=1}^n{\bm{x}_i\bm{x}_i^T \exp{(\bm{x}_i^T\beta)}}
\end{align}
and the Fisher-scoring algorithm is therefore
\begin{equation}
  \bm{\beta}^{(t+1)} = \bm{\beta}^{(t)} + \mathbf{F}(\bm{\beta}^{(t)})^{-1}\bm{s}(\bm{\beta}^{(t)})
\end{equation}
```{r}
fisher_score_iteration <- function(beta, X, y) {
  n <- ncol(X)
  s <- c(t(y - exp(X %*% beta)) %*% X)
  Fisher <- matrix(rep(0, n^2), nrow=n, ncol=n)
  for (i in 1:nrow(X)) { # This is not ideal, but multidimensional arrays are difficult to manage in R
    Fisher <- Fisher + c(exp(X[i, ] %*% beta)) * X[i, ] %o% X[i, ]
  }
  return(beta + solve(Fisher, s))
}
```

Reusing some of the code from `mylm`, we can easily obtain the design matrix $X$ and the response $y$, and run the Fisher scoring algorithm. The algorithm converges very quickly, so 100 iterations should be more than safe. Alternatively, one can implement some stopping criterion based on the absolute difference in $\bm{\beta}^{(t)}$, or based on the likelihood function for $\bm{\beta}$.
```{r}
fit_coefficients <- function(formula, data, contrasts = NULL, ...) {
  # Extract model matrix & responses
  mf <- model.frame(formula = formula, data = data)
  X  <- model.matrix(attr(mf, "terms"), data = mf, contrasts.arg = contrasts)
  y  <- model.response(mf)
  terms <- attr(mf, "terms")

  beta <- rep(0, ncol(X))
  for(i in 1:100) {
    beta <- fisher_score_iteration(beta, X, y)
  }
  return(beta)
}
```

This leads to the following coefficient estimates:
```{r}
my.coeffs <- fit_coefficients(y ~ team + is_home, data=eliteserie.played.transf)
my.coeffs
```


Asserting that they are indeed correct using the built in glm function in R:
```{r}
ref.coeffs <- glm(y ~ team + is_home, data=eliteserie.played.transf, family='poisson')$coefficients
abs(mean(my.coeffs - ref.coeffs))
```
Looks good!

## d)
In order to simulate 1000 matches, we first need to add the coefficient of the reference level to `my.coeffs`.
```{r}
ref_coeff <- eliteserie$home %>% 
  unique() %>% 
  lapply(function(str) glue("team{str}")) %>% 
  unlist() %>% 
  setdiff(names(my.coeffs))

my.coeffs[[ref_coeff]] <- 0
```

Simulation can be done in the following steps:
1. Estimate $\hat{\lambda}$ for each team in each match using the model, i.e.
\begin{align*}
  \hat{\lambda_{i, H}} &= \exp{\left(\beta_0 + \beta_H + \beta_H - \beta_A \right)} \\
  \hat{\lambda_{i, A}} &= \exp{\left(\beta_0 + \beta_A - \beta_H \right)}
\end{align*}
2. For $m$ simulations, and for all $i$ matches, simulate the goals of each team assuming they follow a Poisson distribution with rate $\hat{\lambda}_{i, H}$ and $\hat{\lambda}_{i, A}$ for the home- and away teams  respectively.

We will first implement the function to estimate the rate for each match. Since I have created the model a bit differently, I will do this manually  by retrieving each respective coefficient.
```{r}
estimate_lambda <- function(match) {
  # Args:
  #   match (DataFrame row): Row of the eliteserie dataframe, ya and yh may be excluded
  # Returns:
  #   Same row but with lambda estimates.
  # Retrieving the coefficients
  coeff.home <- my.coeffs[[glue("team{match$home}")]]
  coeff.away <- my.coeffs[[glue("team{match$away}")]]
  
  # Calculating rates
  match$lambda.h <- exp(my.coeffs[["(Intercept)"]] + my.coeffs[["is_homeTRUE"]] + coeff.home - coeff.away)
  match$lambda.a <- exp(my.coeffs[["(Intercept)"]] + coeff.away - coeff.home)
  
  return(match)
}
```

Now applying the above function to each row, giving us coefficient estimates of $\lambda$ for each team in every match.
```{r}
eliteserie.unplayed.est <- eliteserie.unplayed %>%
  rename(lambda.h=yh, lambda.a=ya) %>%
  {lapply(split(., seq_len(nrow(.))), estimate_lambda)} %>%
  {do.call(rbind, .)}
```

Finally, using the estimated rates in `eliteserie.unplayed.est` to simulate 1 000 matches, and then calculate the scroeboard for all simulations.
```{r}
m <- 1000 # Number of  simulations

simulate_dataframe <- function() {
  eliteserie.unplayed.est %>%
    mutate(
      yh=rpois(n(), lambda.h),
      ya=rpois(n(), lambda.a)
    ) %>%
    select(-c(lambda.h, lambda.a))
}
```

```{r}
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores, type="SOCK")
registerDoSNOW(cl)

scores.sim <- foreach(
  i=1:m,
  .packages='tidyverse'
) %dopar%
  {calculate_scores(rbind(eliteserie.played, simulate_dataframe()))} %>%
  bind_rows() %>%
  group_by(team) %>%
  summarize(scores=list(score), score.mean=mean(score), score.sd=sd(score))

stopCluster(cl)
```


With all realizations being saved in the dataframe `scores.sim`, we can easily summarize the result.
```{r}
scores.sim %>%
  mutate(position=rank(-score.mean, ties.method='min')) %>%
  select(-scores) %>%
  merge(scores.played[c('team', 'position')]  %>% rename(position.played=position), by='team') %>%
  mutate(pos_change=position.played - position) %>%
  select(-position.played) %>%
  arrange(position)
```
It is clear that Viking and Bodø/Glimt are leading by a substantial margin. Furthermore, most teams kept their positions as of October 1st, except for Molde and Brann, who switched. The models ability to estimate change in leaderboard based on the teams playing in the future is probably its best strength. The new leaderboard also appropriately positioned Haugesund, Vålerenga, and Sandefjord.

One can also observe that the estimated standard deviation starts out quite small for the worst teams, and steadily increases with increasing score. This is due to the fact that the last 80 matches (33% of the championship) are simulated using a Poisson distribution, and the rest of the data for calculating the score is identical for all simulations. Let $S_i$ be the score for team $i$. The total variance for team $i$ is given by

\begin{equation}
  (\#eq:var)
  \mathrm{Var}(S_i) = \mathrm{Var}(\mathrm{Played}_i) + \mathrm{Var}(\mathrm{Simulated}_i) = 0 + \hat{\lambda}_i = \hat{\lambda}_i
\end{equation}

Where we in \@ref(eq:var) used the fact that the variance of observed variables is obviously 0. And thus, the standard deviation for each team is actually the square root of their expected score increase during the last eighty matches (according to the simulation).

Plotting score distribution histograms for all teams:
```{r, fig.width=20, fig.height=12}
ggplot(scores.sim %>% unnest(cols=scores), aes(x=scores)) +
  geom_histogram(bins=60, fill="skyblue", color=NA) +
  facet_wrap(~ team, scales="free_y") +
  labs(title="Score distribution for each team")
```
The plots above show that the simulated score for each team is approximately normal.
